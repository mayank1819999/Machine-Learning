import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dfx = pd.read_csv('../Datasets/linearX.csv')
dfy = pd.read_csv('../Datasets/linearY.csv')

x = dfx.values
y = dfy.values

x = x.reshape((-1,))
y = y.reshape((-1,))
print(x.shape)
print(y.shape)

plt.scatter(x,y)
#normlize x values
X = (x-x.mean())/x.std()
Y = y
plt.scatter(X,Y)
plt.show()


#Gradient Descent Algorithm
#Start with a random theta(you can take zeroes also)
#Repeat until converge
#Update Theta according to the rule

def hypothesis(x,theta):
    return theta[0] + theta[1]*x


def error(X,Y,theta):
    
    m = X.shape[0]
    error = 0
    
    for i in range(m):
        hx = hypothesis(X[i],theta)
        error += (hx-Y[i])**2
        
    return error

def gradient(X,Y,theta):    
    grad = np.zeros((2,))
    m = X.shape[0]

    for i in range(m):
        hx = hypothesis(X[i],theta)
        grad[0] +=  (hx-Y[i])
        grad[1] += (hx-Y[i])*X[i]
        
    return grad
    
#Algorithm
def gradientDescent(X,Y,learning_rate=0.001):    
    theta = np.array([-2.0,0.0])
    itr = 0
    max_itr = 100
    
    error_list = []
    theta_list = []
    
    while(itr<=max_itr):
        grad = gradient(X,Y,theta)
        e = error(X,Y,theta)
        error_list.append(e)
        
        theta_list.append((theta[0],theta[1]))
        theta[0] = theta[0] - learning_rate*grad[0]
        theta[1] = theta[1] - learning_rate*grad[1]      
        
        itr += 1
           
    return theta,error_list,theta_list
final_theta, error_list,theta_list = gradientDescent(X,Y)
plt.plot(error_list)
plt.show()
print(final_theta)

#time to plot predicted line
### Plot the line for testing data

xtest = np.linspace(-2,6,10)
print(xtest)

plt.scatter(X,Y,label='Training Data')
plt.scatter(xtest,hypothesis(xtest,final_theta),color='orange',label="Prediction")
plt.legend()
plt.show()
