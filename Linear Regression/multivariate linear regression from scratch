import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dfx = pd.read_csv('../Datasets/linearX.csv')
dfy = pd.read_csv('../Datasets/linearY.csv')

x = dfx.values
y = dfy.values

x = x.reshape((-1,))
y = y.reshape((-1,))
print(x.shape)
print(y.shape)

plt.scatter(x,y)
#normlize x values
X = (x-x.mean())/x.std()
Y = y
plt.scatter(X,Y)
plt.show()


#Gradient Descent Algorithm
#Start with a random theta(you can take zeroes also)
#Repeat until converge
#Update Theta according to the rule

def hypothesis(x,theta):
    s=0
    s+=theta[0]
    m=x.shape[1]
    for i in range(1,m):
      s+=theta[i]*x[i]

def error(X,Y,theta):    
    m = X.shape[0]
    error = 0
    
    for i in range(m):
        hx = hypothesis(X[i],theta)
        error += (hx-Y[i])**2
    return error

def gradient(X,Y,theta):    
    
    m = X.shape[0]
    n=X.shape[1]
    grad = np.zeros(n+1)

    for i in range(m):
        hx = hypothesis(X[i],theta)
        for j in range(n)
            grad[j] += (hx-Y[i])*X[i]       
    return grad
    
#Algorithm
def gradientDescent(X,Y,learning_rate=0.001):
    n=x.shape[1]
    theta = np.zeros(n+1)
    itr = 0
    max_itr = 100    
    error_list = []
    theta_list = []
    
    while(itr<=max_itr):
        grad = gradient(X,Y,theta)
        e = error(X,Y,theta)
        error_list.append(e)        
        theta_list.append(theta)
        for i in range(0,n):
            theta[i] = theta[i] - learning_rate*grad[i]
            
        itr += 1
           
    return theta,error_list,theta_list
final_theta, error_list,theta_list = gradientDescent(X,Y)
plt.plot(error_list)
plt.show()
print(final_theta)

#time to plot predicted line
### Plot the line for testing data

xtest = np.linspace(-2,6,10)
print(xtest)

plt.scatter(X,Y,label='Training Data')
plt.scatter(xtest,hypothesis(xtest,final_theta),color='orange',label="Prediction")
plt.legend()
plt.show()
